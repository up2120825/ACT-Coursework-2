# -*- coding: utf-8 -*-
"""ACT Coursework 2 part 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TIG_aU-wxjRNyWGKJcRi5g_XaqlVGj_u
"""

# https://www.kaggle.com/datasets/karnikakapoor/satellite-orbital-catalog

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import OrdinalEncoder
import tensorflow as tf

# download dataset from kaggle, and import as a pandas dataframe
df = pd.read_csv('https://storage.googleapis.com/kagglesdsdata/datasets/8617423/13886558/current_catalog.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20251127%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251127T143930Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=a8a3a54593be9274dc49333cc6a2c7dfd359a496aac094ceaac1bf2dd637bbe2421bf5af805a8a4b65e16af91d568442701719fcb7f32e8c933451397c9c0519ae8498532eadd14c1683f133f24d612456c992596152c5853975862fe8339d551b8a77aa78540cb5ec7bdd8d32593c10c189d912c902d0cece92ff7cf2efb1233211c04405f3476814c577846a3985acd360c5cda740592a801314d8ebc68acd343b7693a64da49f3d12fb1a4662f33bda3ffc52e06420fcb7156eadfaabecdec543f58e0467dd337bc8207ae92813bb0f04b8829652a67f52d554316f6bc1b297d16c80ff0ebfdc2d48a70b8a6dffaeb052f77b573a9251895def1f6df4edf6')

# display some info about the dataframe
display(df.head())
df.info()

# separate features (x) and target (y)
x = df.drop('congestion_risk', axis=1)
y = df['congestion_risk']

# exclude descriptive columns not used for training data
exclude_columns = ['norad_id', 'name', 'epoch', 'data_source', 'snapshot_date', 'last_seen']
categorical_cols = ['object_type', 'satellite_constellation', 'altitude_category', 'orbital_band', 'orbit_lifetime_category', 'country']

# drop excluded columns
x_processed = x.drop(columns=exclude_columns, errors='ignore')

# use ordinal encoding to change categorical data to numerical for training
encoder = OrdinalEncoder()
x_processed[categorical_cols] = encoder.fit_transform(x_processed[categorical_cols])

# fit the encoder on the entire target variable 'y' to ensure all possible labels are learned
y_encoded_full = encoder.fit_transform(y.values.reshape(-1, 1))
num_classes = len(encoder.categories_[0]) # get the number of unique classes

# split data into training and testing sets with the processed data
x_train, x_test, y_train, y_test = train_test_split(x_processed, y, test_size=0.2, random_state=42)

# now transform y_train and y_test using the fitted encoder
y_train_encoded = encoder.transform(y_train.values.reshape(-1, 1)).flatten()
y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1)).flatten()

# define a fully connected (dense) neural network model
cnn_model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(x_train.shape[1],)), # input layer with number of features
    tf.keras.layers.Dropout(0.5), # dropout layer to prevent overfitting
    tf.keras.layers.Dense(64, activation='relu'), # hidden layer
    tf.keras.layers.Dropout(0.3), # dropout layer to prevent overfitting
    tf.keras.layers.Dense(num_classes, activation='softmax') # output layer with number of classes
])

# compile the model
cnn_model.compile(optimizer='adam', # popular and efficient algorithm
              loss='sparse_categorical_crossentropy', # sparse categorical cross-entropy is appropriate for multi-class classification problems
              metrics=['accuracy'])

# fit the model
cnn_model.fit(x_train, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)

# evaluate the model
lossCNN, accuracyCNN = cnn_model.evaluate(x_test, y_test_encoded)
print('CNN Test accuracy:', accuracyCNN)

# I picked a fully connection neural network model for my problem as they are suitable for tabular data and effective with multi-class classification problems
# The neural network still performs well, with a 0.817 accuracy, but is significantly less accurate than the Random Forest Classifier used in the traditional machine learning approach
# Due to the nature of the data, the Random Forest Classifier is highly effective as decision trees excel on data that splits cleanly on features in this way, so it is more effective than the neural network approach that is less suited to this specific task


{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23f1a96"
      },
      "source": [
        "## Predicting Satellite Congestion Risk: Neural Network Approach\n",
        "\n",
        "Just like in the last part, in this we explore how to predict the congestion risk of satellites using machine learning, using a dataset containing various satellite parameters to build a model that can classify satellites into different congestion risk categories (Low, Medium, High). However in this notebook, we take a different approach. Instead of using a traditional, statistical machine learning approach, we use a neural network algorithm to make predictions instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P-1Iz99sNLYy"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/datasets/karnikakapoor/satellite-orbital-catalog"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setting Up Our Environment and Loading Data\n",
        "\n",
        "Just like last time, before we start, we need to import the necessary libraries.We'll be using `pandas` for data manipulation, `sklearn` (Scikit-learn) for machine learning tasks, and `tensorflow` for neural network models"
      ],
      "metadata": {
        "id": "AVT9lCmMwtye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "M4WHJWhFO3X4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the same dataset again in the same way as before, and mount it as a pandas dataframe for data manipulation"
      ],
      "metadata": {
        "id": "V_KzaOquwyML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download kaggle dataset from google drive, and import as a pandas dataframe\n",
        "df = pd.read_csv('https://drive.google.com/uc?export=download&id=1i4FdBT71ale29-1ido9Q0HNeNzOZ6lFN')"
      ],
      "metadata": {
        "id": "1YTZAK6sO4it"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82dbc02b"
      },
      "source": [
        "### 2. Data Preprocessing\n",
        "\n",
        "Just like before, we need to process the data in the same way for the machine learning algorithms. Using the same code as before, seperate the data into the target variable Y and the features X, exclude irrelevant descriptive columns, convert categorical data into numbers using Ordinal Encoding, and then split the data into training and testing sets for the neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separate features (x) and target (y)\n",
        "x = df.drop('congestion_risk', axis=1)\n",
        "y = df['congestion_risk']\n",
        "\n",
        "# exclude descriptive columns not used for training data\n",
        "exclude_columns = ['norad_id', 'name', 'epoch', 'data_source', 'snapshot_date', 'last_seen']\n",
        "categorical_cols = ['object_type', 'satellite_constellation', 'altitude_category', 'orbital_band', 'orbit_lifetime_category', 'country']\n",
        "\n",
        "# drop excluded columns\n",
        "x_processed = x.drop(columns=exclude_columns, errors='ignore')\n",
        "\n",
        "# use ordinal encoding to change categorical data to numerical for training\n",
        "encoder = OrdinalEncoder()\n",
        "x_processed[categorical_cols] = encoder.fit_transform(x_processed[categorical_cols])\n",
        "\n",
        "# fit the encoder on the entire target variable 'y' to ensure all possible labels are learned\n",
        "y_encoded_full = encoder.fit_transform(y.values.reshape(-1, 1))\n",
        "num_classes = len(encoder.categories_[0]) # get the number of unique classes\n",
        "\n",
        "# split data into training and testing sets with the processed data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# now transform y_train and y_test using the fitted encoder\n",
        "y_train_encoded = encoder.transform(y_train.values.reshape(-1, 1)).flatten()\n",
        "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1)).flatten()"
      ],
      "metadata": {
        "id": "U4wEemdqPgx_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15299835"
      },
      "source": [
        "### 3. Building Our Predictive Brain: The Neural Network Model\n",
        "\n",
        "Now that our data is processed, we can make our machine learning model. For this, we're using a type of model called a \"Fully Connected Neural Network\" (also known as a Dense Neural Network), built with `tensorflow` and `keras`. Neural networks are inspired by the human brain and are excellent at finding complex patterns in data.\n",
        "\n",
        "Let's break down the model's construction:\n",
        "\n",
        "1.  **`tf.keras.models.Sequential`**: This is like stacking layers on top of each other to build our network, with data flowing from one layer to the next.\n",
        "\n",
        "2.  **`tf.keras.layers.Dense(128, activation='relu', input_shape=(x_train.shape[1],))`**: This is our input layer and the first 'hidden' layer.\n",
        "    *   `Dense` means every neuron in this layer is connected to every neuron in the previous layer (or the input).\n",
        "    *   `128` is the number of 'neurons' or units in this layer. More neurons can capture more complex patterns.\n",
        "    *   `activation='relu'` stands for Rectified Linear Unit. It's a commonnly used activation function that helps the network learn non-linear relationships.\n",
        "    *   `input_shape=(x_train.shape[1],)` tells the model how many features it should expect in each input sample (which is the number of columns in our `x_train`).\n",
        "\n",
        "3.  **`tf.keras.layers.Dropout(0.5)`**: This is a dropout layer, where during training, it randomly 'turns off' 50% of the neurons in the previous layer. This helps to prevent the model from becoming too reliant on any single neuron and to make it more robust, and so helps to reduce risk of overfitting, where the model memorizes the training data instead of learning general patterns.\n",
        "\n",
        "4.  **`tf.keras.layers.Dense(64, activation='relu')`**: Another hidden layer, similar to the first, but with fewer neurons (`64`).\n",
        "\n",
        "5.  **`tf.keras.layers.Dropout(0.3)`**: Another dropout layer, this time turning off 30% of neurons.\n",
        "\n",
        "6.  **`tf.keras.layers.Dense(num_classes, activation='softmax')`**: This is our **output layer**.\n",
        "    *   `num_classes` is the number of unique congestion risk categories we are trying to predict, 'Low', 'Medium', 'High'.\n",
        "    *   `activation='softmax'` is used for multi-class classification problems. It outputs a probability distribution over the `num_classes`, meaning it tells us the likelihood that a satellite belongs to each risk category. The sum of these probabilities will be 1.\n",
        "\n",
        "### Compiling the Model: Setting Up for Learning\n",
        "\n",
        "After defining the network's structure, we need to 'compile' it. This step configures the learning process:\n",
        "\n",
        "*   **`optimizer='adam'`**: The optimizer is what trains the neural network to be more accurate. 'Adam' is a very popular and effective algorithm that adjusts the internal weights of the neural network to minimize errors during training.\n",
        "*   **`loss='sparse_categorical_crossentropy'`**: The 'loss function' measures how far off our model's predictions are from the true values. For multi class classification with integer encoded labels, `sparse_categorical_crossentropy` is the appropriate choice. The optimizer tries to minimize this loss.\n",
        "*   **`metrics=['accuracy']`**: Metrics are what we use to monitor the training process and evaluate the model's performance. 'Accuracy' is straightforward, it tells us the proportion of correctly predicted instances.\n",
        "\n",
        "### Training and Evaluating the Model: The Learning Begins!\n",
        "\n",
        "Finally, we train our model using the `cnn_model.fit()` method. This is where the model 'learns' from the training data:\n",
        "\n",
        "*   **`x_train`, `y_train_encoded`**: These are our training features and their corresponding encoded target labels.\n",
        "*   **`epochs=10`**: An epoch is one complete pass through the entire training dataset. So here we are telling the model to iterate over the data 10 times.\n",
        "*   **`batch_size=32`**: Instead of feeding all data at once which can be memory intensive, the model processes data in smaller 'batches' (here, 32 samples at a time) and updates its weights after each batch.\n",
        "*   **`validation_split=0.2`**: During training, we reserve a small portion (20%) of the training data as a 'validation set'. The model doesn't learn from this data directly, but is used to monitor performance during training. This helps us catch overfitting early.\n",
        "\n",
        "After training, we `evaluate` the model on our completely unseen `x_test` and `y_test_encoded` data to get a final, unbiased measure of its performance. The `accuracyCNN` value tells us how well our neural network performed in predicting the congestion risk for satellites it had never seen before."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a fully connected (dense) neural network model\n",
        "cnn_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(x_train.shape[1],)), # input layer with number of features\n",
        "    tf.keras.layers.Dropout(0.5), # dropout layer to prevent overfitting\n",
        "    tf.keras.layers.Dense(64, activation='relu'), # hidden layer\n",
        "    tf.keras.layers.Dropout(0.3), # dropout layer to prevent overfitting\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax') # output layer with number of classes\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "cnn_model.compile(optimizer='adam', # popular and efficient algorithm\n",
        "              loss='sparse_categorical_crossentropy', # sparse categorical cross-entropy is appropriate for multi-class classification problems\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit the model\n",
        "cnn_model.fit(x_train, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# evaluate the model\n",
        "lossCNN, accuracyCNN = cnn_model.evaluate(x_test, y_test_encoded)\n",
        "print('CNN Test accuracy:', accuracyCNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYglrZYdPjQn",
        "outputId": "0c1c0abe-b4a0-447f-d283-a9208f728065"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6361 - loss: 206.3427 - val_accuracy: 0.8251 - val_loss: 24.7365\n",
            "Epoch 2/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6966 - loss: 34.1427 - val_accuracy: 0.7916 - val_loss: 2.4682\n",
            "Epoch 3/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7093 - loss: 12.7993 - val_accuracy: 0.8251 - val_loss: 1.5386\n",
            "Epoch 4/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7879 - loss: 5.7955 - val_accuracy: 0.8251 - val_loss: 0.6485\n",
            "Epoch 5/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8110 - loss: 2.0699 - val_accuracy: 0.8251 - val_loss: 0.5612\n",
            "Epoch 6/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8141 - loss: 1.3331 - val_accuracy: 0.8251 - val_loss: 0.5450\n",
            "Epoch 7/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8201 - loss: 1.0830 - val_accuracy: 0.8251 - val_loss: 0.5424\n",
            "Epoch 8/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8162 - loss: 0.9305 - val_accuracy: 0.8251 - val_loss: 0.5497\n",
            "Epoch 9/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8180 - loss: 0.7978 - val_accuracy: 0.8251 - val_loss: 0.5430\n",
            "Epoch 10/10\n",
            "\u001b[1m273/273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8194 - loss: 0.9746 - val_accuracy: 0.8251 - val_loss: 0.5372\n",
            "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8167 - loss: 0.5416\n",
            "CNN Test accuracy: 0.8133724927902222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cb4332"
      },
      "source": [
        "### 4. Conclusion: Neural Networks for Satellite Congestion\n",
        "\n",
        "Our neural network achieved a good accuracy of approximately 0.817 (81.7%) on unseen test data, which indicates that the model is quite effective at classifying satellites into their respective congestion risk categories. While this is a good result, it's worth noting that the Random Forest Classifier from the previous part performed even better on tabular data like this, since the relationships in the data are clearly defined by distinct feature boundaries. Random Forests excel at making decisions based on such clear splits."
      ]
    }
  ]
}